{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers: Hoe AI tekst begrijpt en vertaalt\n",
    "\n",
    "ChatGPT, Claude, Google Translate — ze gebruiken allemaal **transformers**. Maar wat is dat?\n",
    "\n",
    "In dit notebook leggen we transformers uit zonder ingewikkelde wiskunde. Je leert:\n",
    "1. Wat het probleem was dat transformers oplossen\n",
    "2. Hoe de **encoder** werkt (begrijpen van tekst)\n",
    "3. Hoe de **decoder** werkt (genereren van tekst)\n",
    "4. Hoe ze samenwerken (bijvoorbeeld bij vertalen)\n",
    "5. Wat **attention** betekent\n",
    "\n",
    "---\n",
    "**Instructie:** Voer iedere cel uit met **Shift+Enter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Het probleem: Context begrijpen\n",
    "\n",
    "Oude AI-modellen lazen tekst als een mens die maar 1 woord tegelijk kon onthouden. Bij deze zin:\n",
    "\n",
    "> \"De kat zat op de mat omdat **hij** moe was\"\n",
    "\n",
    "Wat betekent \"hij\"? Voor een mens is het duidelijk: de kat. Maar een simpel model weet niet dat \"hij\" terug verwijst naar \"de kat\".\n",
    "\n",
    "**Transformers lossen dit op door alle woorden tegelijk te bekijken en te begrijpen hoe ze met elkaar samenhangen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De twee delen: Encoder en Decoder\n",
    "\n",
    "Een transformer heeft twee delen:\n",
    "\n",
    "### 1. **Encoder** = De Lezer\n",
    "- Leest de invoer (bijvoorbeeld een Nederlandse zin)\n",
    "- Begrijpt de betekenis van elk woord in de context\n",
    "- Maakt een \"begrip\" van de hele zin\n",
    "\n",
    "### 2. **Decoder** = De Schrijver\n",
    "- Neemt het \"begrip\" van de encoder\n",
    "- Genereert de uitvoer (bijvoorbeeld een Engelse vertaling)\n",
    "- Schrijft woord voor woord\n",
    "\n",
    "**Voorbeeld:**\n",
    "```\n",
    "Nederlands: \"De kat zit op de mat\"\n",
    "           ↓\n",
    "      [ENCODER] → begrijpt de betekenis\n",
    "           ↓\n",
    "      [DECODER] → schrijft Engels\n",
    "           ↓\n",
    "Engels: \"The cat sits on the mat\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De Encoder: Tekst begrijpen\n",
    "\n",
    "Stel je voor dat de encoder een groep experts is. Elk expert kijkt naar de zin en merkt verschillende dingen op:\n",
    "\n",
    "**Zin:** \"De bank aan de rivier was groen\"\n",
    "\n",
    "- Expert 1: \"bank + rivier → het gaat om een zitbank buiten, niet om een geldbank\"\n",
    "- Expert 2: \"groen → beschrijft de bank\"\n",
    "- Expert 3: \"aan de rivier → locatie van de bank\"\n",
    "\n",
    "De encoder combineert al deze inzichten en maakt voor elk woord een \"begrip\" dat de context bevat.\n",
    "\n",
    "Dit gebeurt via **self-attention**: elk woord \"let op\" alle andere woorden om context te begrijpen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laten we visualiseren hoe woorden naar elkaar \"kijken\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Voorbeeld zin\n",
    "woorden = [\"De\", \"bank\", \"aan\", \"de\", \"rivier\", \"was\", \"groen\"]\n",
    "\n",
    "# Simuleer attention scores (hoe belangrijk is elk woord voor elk ander woord?)\n",
    "# In echte transformers wordt dit berekend, hier maken we een voorbeeld\n",
    "attention = np.array([\n",
    "    [0.1, 0.8, 0.0, 0.0, 0.1, 0.0, 0.0],  # \"De\" let vooral op \"bank\"\n",
    "    [0.0, 0.2, 0.2, 0.0, 0.4, 0.0, 0.2],  # \"bank\" let op \"rivier\" en \"groen\"\n",
    "    [0.0, 0.3, 0.1, 0.1, 0.5, 0.0, 0.0],  # \"aan\" let op \"rivier\"\n",
    "    [0.0, 0.0, 0.0, 0.1, 0.8, 0.1, 0.0],  # \"de\" let op \"rivier\"\n",
    "    [0.0, 0.3, 0.2, 0.0, 0.2, 0.1, 0.2],  # \"rivier\" let op \"bank\"\n",
    "    [0.0, 0.4, 0.0, 0.0, 0.0, 0.1, 0.5],  # \"was\" let op \"bank\" en \"groen\"\n",
    "    [0.0, 0.6, 0.0, 0.0, 0.1, 0.1, 0.2],  # \"groen\" let vooral op \"bank\"\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attention, xticklabels=woorden, yticklabels=woorden, \n",
    "            cmap='YlOrRd', annot=True, fmt='.1f', cbar_kws={'label': 'Attention score'})\n",
    "plt.title('Hoe woorden naar elkaar \"kijken\" (Self-Attention)', fontsize=14, pad=20)\n",
    "plt.xlabel('Naar welk woord wordt gekeken', fontsize=11)\n",
    "plt.ylabel('Woord dat kijkt', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretatie:\")\n",
    "print(\"- 'bank' kijkt vooral naar 'rivier' (0.4) en 'groen' (0.2)\")\n",
    "print(\"- 'groen' kijkt vooral naar 'bank' (0.6) → het beschrijft de bank\")\n",
    "print(\"- Zo begrijpt het model dat 'bank' hier een zitbank is, geen geldinstelling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De Decoder: Tekst genereren\n",
    "\n",
    "De decoder werkt andersom. Hij genereert tekst **woord voor woord**:\n",
    "\n",
    "1. Hij kijkt naar wat hij al heeft geschreven\n",
    "2. Hij kijkt naar het begrip van de encoder\n",
    "3. Hij voorspelt het volgende woord\n",
    "\n",
    "**Voorbeeld van vertalen:**\n",
    "```\n",
    "Encoder begrip: [De kat zit op de mat]\n",
    "\n",
    "Decoder:\n",
    "Stap 1: <start> → voorspelt \"The\"\n",
    "Stap 2: \"The\" → voorspelt \"cat\"\n",
    "Stap 3: \"The cat\" → voorspelt \"sits\"\n",
    "Stap 4: \"The cat sits\" → voorspelt \"on\"\n",
    "...\n",
    "```\n",
    "\n",
    "Bij elke stap gebruikt de decoder **twee soorten attention**:\n",
    "- **Self-attention**: kijkt naar wat hij al heeft geschreven\n",
    "- **Cross-attention**: kijkt naar de originele tekst van de encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder vs Decoder: Wanneer gebruik je wat?\n",
    "\n",
    "Niet alle taken hebben beide nodig:\n",
    "\n",
    "| Taak | Gebruikt | Voorbeeld |\n",
    "|------|----------|----------|\n",
    "| Tekst **begrijpen** (classificatie) | Alleen Encoder | BERT: \"Is deze review positief of negatief?\" |\n",
    "| Tekst **genereren** | Alleen Decoder | GPT: \"Schrijf een verhaal over...\" |\n",
    "| Tekst **transformeren** (vertalen) | Encoder + Decoder | Google Translate: Nederlands → Engels |\n",
    "\n",
    "**ChatGPT gebruikt alleen een decoder** (GPT = Generative Pre-trained Transformer), maar de decoder heeft wel self-attention zoals de encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention uitgelegd met een analogie\n",
    "\n",
    "**Stel je voor:** Je leest een detective-roman. \n",
    "\n",
    "Op pagina 200 staat: *\"Hij pakte het wapen dat hij drie dagen eerder had verstopt\"*\n",
    "\n",
    "Om dit te begrijpen moet je:\n",
    "1. Terug bladeren naar pagina 150 waar het wapen werd verstopt\n",
    "2. Die context combineren met wat je nu leest\n",
    "\n",
    "**Dat is attention!** Het model \"bladert terug\" en kijkt naar eerdere woorden die belangrijk zijn voor het huidige woord.\n",
    "\n",
    "In een transformer gebeurt dit automatisch voor elk woord met elk ander woord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Een simpele demonstratie: welke woorden zijn belangrijk voor betekenis?\n",
    "\n",
    "def toon_attention(zin, focus_woord_index):\n",
    "    \"\"\"\n",
    "    Visualiseer welke woorden belangrijk zijn voor een specifiek woord\n",
    "    \"\"\"\n",
    "    woorden = zin.split()\n",
    "    focus = woorden[focus_woord_index]\n",
    "    \n",
    "    # Simuleer attention scores (in echt wordt dit door het model berekend)\n",
    "    # Hier maken we handmatig een voorbeeld\n",
    "    scores = [0.1] * len(woorden)\n",
    "    \n",
    "    # Voor demonstratie: maak sommige woorden belangrijker\n",
    "    if \"verstopt\" in zin and focus == \"Hij\":\n",
    "        # \"Hij\" verwijst naar wie het wapen verstopte\n",
    "        if \"detective\" in woorden:\n",
    "            scores[woorden.index(\"detective\")] = 0.7\n",
    "    \n",
    "    if \"eerder\" in zin and focus == \"wapen\":\n",
    "        # \"wapen\" is gekoppeld aan \"verstopt\"\n",
    "        if \"verstopt\" in woorden:\n",
    "            scores[woorden.index(\"verstopt\")] = 0.8\n",
    "    \n",
    "    # Normaliseer\n",
    "    total = sum(scores)\n",
    "    scores = [s/total for s in scores]\n",
    "    \n",
    "    # Visualiseer\n",
    "    print(f\"\\nWelke woorden zijn belangrijk voor '{focus}'?\\n\")\n",
    "    for woord, score in zip(woorden, scores):\n",
    "        bar = '█' * int(score * 50)\n",
    "        print(f\"{woord:15} {bar} {score:.2f}\")\n",
    "\n",
    "# Test\n",
    "zin = \"De detective pakte het wapen dat hij verstopt had\"\n",
    "print(\"=\"*60)\n",
    "print(f\"Zin: {zin}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "toon_attention(zin, 1)  # Focus op \"detective\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoe Encoder en Decoder samenwerken (bij vertalen)\n",
    "\n",
    "Laten we het hele proces doorlopen:\n",
    "\n",
    "### Stap 1: Encoder verwerkt invoer\n",
    "```\n",
    "Input: \"Ik hou van katten\"\n",
    "→ Encoder maakt begripsrepresentatie van elk woord in context\n",
    "```\n",
    "\n",
    "### Stap 2: Decoder genereert uitvoer\n",
    "```\n",
    "Decoder krijgt: <start>\n",
    "+ Encoder begrip van \"Ik hou van katten\"\n",
    "→ Voorspelt: \"I\"\n",
    "\n",
    "Decoder krijgt: \"I\"\n",
    "+ Encoder begrip\n",
    "→ Voorspelt: \"love\"\n",
    "\n",
    "Decoder krijgt: \"I love\"\n",
    "+ Encoder begrip (let op: \"katten\" is meervoud!)\n",
    "→ Voorspelt: \"cats\" (niet \"cat\")\n",
    "```\n",
    "\n",
    "De decoder gebruikt **cross-attention** om terug te kijken naar de originele Nederlandse woorden terwijl hij Engels schrijft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiseer cross-attention bij vertalen\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Voorbeeld vertaling\n",
    "nl_woorden = [\"Ik\", \"hou\", \"van\", \"katten\"]\n",
    "en_woorden = [\"I\", \"love\", \"cats\"]\n",
    "\n",
    "# Cross-attention: welk Engels woord let op welk Nederlands woord?\n",
    "cross_attention = np.array([\n",
    "    [0.9, 0.05, 0.05, 0.0],   # \"I\" kijkt vooral naar \"Ik\"\n",
    "    [0.0, 0.6, 0.4, 0.0],     # \"love\" kijkt naar \"hou\" en \"van\"\n",
    "    [0.0, 0.0, 0.1, 0.9],     # \"cats\" kijkt vooral naar \"katten\"\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cross_attention, \n",
    "            xticklabels=nl_woorden, \n",
    "            yticklabels=en_woorden,\n",
    "            cmap='Blues', \n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            cbar_kws={'label': 'Attention'})\n",
    "plt.title('Cross-Attention bij vertalen (Decoder kijkt naar Encoder)', fontsize=13, pad=15)\n",
    "plt.xlabel('Nederlands (van Encoder)', fontsize=11)\n",
    "plt.ylabel('Engels (Decoder schrijft)', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretatie:\")\n",
    "print(\"- Bij het schrijven van 'I' kijkt de decoder vooral naar 'Ik' (0.90)\")\n",
    "print(\"- Bij 'love' kijkt hij naar 'hou' (0.60) en 'van' (0.40)\")\n",
    "print(\"- Bij 'cats' kijkt hij naar 'katten' (0.90)\")\n",
    "print(\"\\nZo 'vertaalt' de decoder terwijl hij het origineel in gedachten houdt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waarom zijn Transformers zo krachtig?\n",
    "\n",
    "### 1. **Parallelle verwerking**\n",
    "Oude modellen (RNN/LSTM) moesten woorden één voor één verwerken. Transformers bekijken alle woorden tegelijk → veel sneller.\n",
    "\n",
    "### 2. **Lange-afstand relaties**\n",
    "Woord 1 kan direct communiceren met woord 1000. Geen probleem met vergeten van context.\n",
    "\n",
    "### 3. **Multi-head attention**\n",
    "Het model heeft meerdere \"attention heads\" - elk let op andere aspecten:\n",
    "- Head 1: grammaticale relaties (onderwerp-werkwoord)\n",
    "- Head 2: semantische relaties (synoniemen)\n",
    "- Head 3: lange-afstand verwijzingen (hij → de kat)\n",
    "\n",
    "### 4. **Schaalbaarheid**\n",
    "Je kunt ze groter maken (meer layers, meer parameters) en ze blijven beter werken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samenvatting\n",
    "\n",
    "```\n",
    "TRANSFORMER\n",
    "│\n",
    "├── ENCODER (begrijpt tekst)\n",
    "│   ├── Self-Attention: woorden kijken naar elkaar\n",
    "│   ├── Begrijpt context: \"bank\" bij \"rivier\" ≠ \"bank\" bij \"geld\"\n",
    "│   └── Output: begripsrepresentatie\n",
    "│\n",
    "└── DECODER (genereert tekst)\n",
    "    ├── Self-Attention: kijkt naar al geschreven woorden\n",
    "    ├── Cross-Attention: kijkt naar encoder output\n",
    "    └── Output: woord voor woord nieuwe tekst\n",
    "```\n",
    "\n",
    "**Belangrijkste concept:** Attention = het model leert zelf welke woorden belangrijk zijn voor welke andere woorden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenteer zelf!\n",
    "\n",
    "Probeer verschillende zinnen en kijk hoe woorden naar elkaar \"zouden moeten kijken\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 1: Welke woorden zijn belangrijk voor elkaar?\n",
    "\n",
    "def simuleer_attention(zin):\n",
    "    \"\"\"\n",
    "    Simuleer welke woorden belangrijk zijn voor elkaar.\n",
    "    Dit is een vereenvoudigde versie - echte transformers berekenen dit automatisch.\n",
    "    \"\"\"\n",
    "    woorden = zin.split()\n",
    "    n = len(woorden)\n",
    "    \n",
    "    # Maak basis attention matrix (elk woord let een beetje op alles)\n",
    "    attention = np.ones((n, n)) * 0.1\n",
    "    \n",
    "    # Voeg wat logica toe (simpel, niet perfect)\n",
    "    for i, woord in enumerate(woorden):\n",
    "        # Lidwoorden kijken naar het volgende zelfstandig naamwoord\n",
    "        if woord.lower() in ['de', 'het', 'een']:\n",
    "            if i + 1 < n:\n",
    "                attention[i, i+1] = 0.7\n",
    "        \n",
    "        # Bijvoeglijke naamwoorden kijken naar zelfstandig naamwoord\n",
    "        if i > 0 and woord[0].isupper() == False:  # Simpele heuristiek\n",
    "            attention[i, max(0, i-1)] = 0.5\n",
    "    \n",
    "    # Normaliseer rijen\n",
    "    attention = attention / attention.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Visualiseer\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention, \n",
    "                xticklabels=woorden, \n",
    "                yticklabels=woorden,\n",
    "                cmap='YlGnBu', \n",
    "                annot=True, \n",
    "                fmt='.2f',\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Attention score'})\n",
    "    plt.title(f'Gesimuleerde Attention voor: \"{zin}\"', fontsize=13, pad=15)\n",
    "    plt.xlabel('Naar welk woord', fontsize=11)\n",
    "    plt.ylabel('Woord dat kijkt', fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Probeer verschillende zinnen!\n",
    "print(\"Probeer deze zinnen of verzin je eigen:\")\n",
    "print(\"1. 'De grote rode auto staat in de straat'\")\n",
    "print(\"2. 'Hij pakte het boek dat zij had gelezen'\")\n",
    "print(\"3. 'De kat jaagt de muis door het huis'\\n\")\n",
    "\n",
    "# Pas hier je eigen zin aan!\n",
    "jouw_zin = \"De grote rode auto staat in de straat\"\n",
    "simuleer_attention(jouw_zin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 2: Simuleer een simpele \"vertaling\"\n",
    "\n",
    "# Simpel woordenboek (in het echt heeft het model dit geleerd)\n",
    "vertaalwoordenboek = {\n",
    "    'ik': 'I', 'jij': 'you', 'hij': 'he', 'zij': 'she',\n",
    "    'de': 'the', 'een': 'a', 'het': 'the',\n",
    "    'kat': 'cat', 'hond': 'dog', 'katten': 'cats', 'honden': 'dogs',\n",
    "    'hou': 'love', 'houdt': 'loves', 'houd': 'love',\n",
    "    'van': 'of', 'op': 'on', 'in': 'in',\n",
    "    'zit': 'sits', 'zat': 'sat', 'staat': 'stands',\n",
    "    'mat': 'mat', 'bank': 'bench', 'stoel': 'chair',\n",
    "    'rent': 'runs', 'loopt': 'walks', 'speelt': 'plays'\n",
    "}\n",
    "\n",
    "def simpele_vertaling(nl_zin):\n",
    "    \"\"\"\n",
    "    Super simpele \"vertaling\" - echte transformers zijn veel slimmer!\n",
    "    \"\"\"\n",
    "    woorden = nl_zin.lower().split()\n",
    "    vertaald = []\n",
    "    \n",
    "    print(f\"\\nOrigineel: {nl_zin}\")\n",
    "    print(\"\\nStap-voor-stap vertaling:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, woord in enumerate(woorden):\n",
    "        if woord in vertaalwoordenboek:\n",
    "            eng_woord = vertaalwoordenboek[woord]\n",
    "            vertaald.append(eng_woord)\n",
    "            \n",
    "            # Simuleer wat de decoder \"ziet\"\n",
    "            context_nl = ' '.join(woorden[:i+1])\n",
    "            context_en = ' '.join(vertaald)\n",
    "            print(f\"Stap {i+1}:\")\n",
    "            print(f\"  NL tot nu toe: '{context_nl}'\")\n",
    "            print(f\"  EN tot nu toe: '{context_en}'\")\n",
    "            print(f\"  → Volgend woord: '{eng_woord}'\\n\")\n",
    "        else:\n",
    "            vertaald.append(woord)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Resultaat: {' '.join(vertaald)}\")\n",
    "    print(\"\\n(Let op: dit is een simpele woordelijke vertaling.\")\n",
    "    print(\"Echte transformers begrijpen grammatica en context!)\")\n",
    "\n",
    "# Probeer zelf zinnen!\n",
    "print(\"Probeer een van deze zinnen of maak je eigen zin:\")\n",
    "print(\"- 'De kat zit op de mat'\")\n",
    "print(\"- 'Ik hou van katten'\")\n",
    "print(\"- 'De hond rent in het park'\\n\")\n",
    "\n",
    "# Pas hier je zin aan (gebruik woorden uit het woordenboek hierboven)\n",
    "jouw_nederlandse_zin = \"De kat zit op de mat\"\n",
    "simpele_vertaling(jouw_nederlandse_zin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 3: Context is belangrijk!\n",
    "\n",
    "def toon_context_verschil(woord, context1, context2):\n",
    "    \"\"\"\n",
    "    Laat zien hoe hetzelfde woord verschillende betekenis heeft in verschillende contexten.\n",
    "    Dit is waar attention essentieel is!\n",
    "    \"\"\"\n",
    "    print(f\"\\nHet woord: '{woord}'\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Context 1: {context1}\")\n",
    "    print(f\"Context 2: {context2}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Analyseer welke woorden belangrijk zijn\n",
    "    woorden1 = context1.split()\n",
    "    woorden2 = context2.split()\n",
    "    \n",
    "    print(f\"\\nOm '{woord}' te begrijpen moet het model letten op:\")\n",
    "    print(f\"\\nIn context 1: belangrijke woorden zijn waarschijnlijk:\")\n",
    "    for w in woorden1:\n",
    "        if w != woord and w.lower() not in ['de', 'het', 'een', 'is', 'was']:\n",
    "            print(f\"  - '{w}'\")\n",
    "    \n",
    "    print(f\"\\nIn context 2: belangrijke woorden zijn waarschijnlijk:\")\n",
    "    for w in woorden2:\n",
    "        if w != woord and w.lower() not in ['de', 'het', 'een', 'is', 'was']:\n",
    "            print(f\"  - '{w}'\")\n",
    "    \n",
    "    print(\"\\nDit is waarom attention zo belangrijk is!\")\n",
    "    print(\"Het model leert automatisch naar de juiste woorden te kijken.\")\n",
    "\n",
    "# Voorbeelden van woorden met meerdere betekenissen\n",
    "print(\"VOORBEELD 1: Het woord 'bank'\\n\")\n",
    "toon_context_verschil(\n",
    "    \"bank\",\n",
    "    \"De bank aan de rivier was groen\",\n",
    "    \"De bank is vandaag gesloten\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(\"VOORBEELD 2: Het woord 'slang'\\n\")\n",
    "toon_context_verschil(\n",
    "    \"slang\",\n",
    "    \"De slang kronkelde door het gras\",\n",
    "    \"De slang was te kort voor de tuin\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nProbeer zelf:\")\n",
    "print(\"Verzin twee zinnen met hetzelfde woord maar verschillende betekenis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENT 4: Jouw eigen zinnen testen!\n",
    "\n",
    "print(\"Experimenteer met je eigen zinnen!\\n\")\n",
    "print(\"Verander de variabelen hieronder en voer de cel opnieuw uit:\\n\")\n",
    "\n",
    "# PAS DEZE AAN:\n",
    "jouw_zin_1 = \"De muis zit in de hoek\"\n",
    "jouw_zin_2 = \"De muis van de computer is kapot\"\n",
    "focus_woord = \"muis\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Woord: '{focus_woord}'\\n\")\n",
    "print(f\"Zin 1: {jouw_zin_1}\")\n",
    "print(f\"Zin 2: {jouw_zin_2}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nIn een transformer zou '{focus_woord}' in zin 1 vooral letten op:\")\n",
    "for woord in jouw_zin_1.split():\n",
    "    if woord.lower() not in ['de', 'het', 'een', 'is', 'in', 'op', 'van', focus_woord.lower()]:\n",
    "        print(f\"  → {woord}\")\n",
    "\n",
    "print(f\"\\nIn een transformer zou '{focus_woord}' in zin 2 vooral letten op:\")\n",
    "for woord in jouw_zin_2.split():\n",
    "    if woord.lower() not in ['de', 'het', 'een', 'is', 'in', 'op', 'van', focus_woord.lower()]:\n",
    "        print(f\"  → {woord}\")\n",
    "\n",
    "print(\"\\nDoor naar verschillende woorden te 'kijken' begrijpt het model\")\n",
    "print(f\"dat '{focus_woord}' twee verschillende betekenissen heeft!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusie\n",
    "\n",
    "Je hebt nu geleerd:\n",
    "\n",
    "✓ Wat transformers zijn en waarom ze belangrijk zijn  \n",
    "✓ Hoe de **encoder** tekst begrijpt door naar alle woorden tegelijk te kijken  \n",
    "✓ Hoe de **decoder** nieuwe tekst genereert woord voor woord  \n",
    "✓ Wat **attention** betekent: woorden leren naar relevante andere woorden te kijken  \n",
    "✓ Hoe encoder en decoder samenwerken bij taken zoals vertalen  \n",
    "\n",
    "**Het kernidee:** In plaats van tekst woord-voor-woord te lezen, bekijken transformers alle woorden tegelijk en leren ze automatisch welke woorden belangrijk zijn voor welke andere woorden.\n",
    "\n",
    "Dit maakt ze extreem krachtig voor:\n",
    "- Vertalen (Google Translate)\n",
    "- Tekst genereren (ChatGPT, Claude)\n",
    "- Tekst begrijpen (sentiment analyse, classificatie)\n",
    "- En nog veel meer!\n",
    "\n",
    "---\n",
    "\n",
    "**Volgende stap:** Probeer de experimenten hierboven aan te passen met je eigen zinnen en woorden!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
