{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÇÔ∏è Tokenization: Hoe AI tekst leest\n",
    "\n",
    "Taalmodellen zoals ChatGPT, Claude en Gemini lezen geen woorden ‚Äî ze lezen **tokens**.\n",
    "\n",
    "Een token is een stukje tekst: soms een heel woord, soms een deel van een woord, soms een leesteken.\n",
    "\n",
    "In dit notebook ontdek je:\n",
    "1. Hoe tekst wordt opgesplitst in tokens\n",
    "2. Waarom sommige woorden in stukjes worden geknipt\n",
    "3. Hoe padding en truncation werken\n",
    "\n",
    "---\n",
    "**Instructie:** Voer iedere cel uit met **Shift+Enter**\n",
    "\n",
    "‚ö†Ô∏è De eerste cel kan even duren (~30 seconden) omdat er een tokenizer wordt gedownload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installeer de benodigde library (duurt even bij eerste keer)\n",
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 1: Tekst tokenizen\n",
    "\n",
    "We laden de **BERT tokenizer** ‚Äî dezelfde tokenizer die gebruikt wordt in veel AI-modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sybra\\OneDrive\\Bureaublad\\neural_nets\\neural_networks_training_advanced_ai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "c:\\Users\\sybra\\OneDrive\\Bureaublad\\neural_nets\\neural_networks_training_advanced_ai\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sybra\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originele tekst:  PyTorch and Hugging Face make deep learning simple.\n",
      "Aantal woorden:   8\n",
      "\n",
      "Tokens:           ['p', '##yt', '##or', '##ch', 'and', 'hugging', 'face', 'make', 'deep', 'learning', 'simple', '.']\n",
      "Aantal tokens:    12\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Laad de BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Probeer een zin\n",
    "tekst = \"PyTorch and Hugging Face make deep learning simple.\"\n",
    "\n",
    "# Tokenize!\n",
    "tokens = tokenizer.tokenize(tekst)\n",
    "\n",
    "print(f\"Originele tekst:  {tekst}\")\n",
    "print(f\"Aantal woorden:   {len(tekst.split())}\")\n",
    "print(f\"\\nTokens:           {tokens}\")\n",
    "print(f\"Aantal tokens:    {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wat valt je op?\n",
    "\n",
    "- `\"PyTorch\"` is opgesplitst in `['p', '##yt', '##or', '##ch']` ‚Äî het model kent het woord niet als geheel\n",
    "- `\"##\"` betekent: dit stukje hoort bij het vorige token\n",
    "- Bekende woorden zoals `\"and\"`, `\"make\"`, `\"deep\"` blijven heel\n",
    "- De punt `.` is een apart token\n",
    "\n",
    "## Stap 2: Van tokens naar getallen\n",
    "\n",
    "Het neural network werkt niet met tekst maar met **getallen**. Elk token heeft een uniek ID-nummer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ‚Üí ID-nummer:\n",
      "-----------------------------------\n",
      "  p               ‚Üí 1052\n",
      "  ##yt            ‚Üí 22123\n",
      "  ##or            ‚Üí 2953\n",
      "  ##ch            ‚Üí 2818\n",
      "  and             ‚Üí 1998\n",
      "  hugging         ‚Üí 17662\n",
      "  face            ‚Üí 2227\n",
      "  make            ‚Üí 2191\n",
      "  deep            ‚Üí 2784\n",
      "  learning        ‚Üí 4083\n",
      "  simple          ‚Üí 3722\n",
      "  .               ‚Üí 1012\n",
      "\n",
      "BERT heeft een vocabulaire van 30,522 tokens!\n"
     ]
    }
   ],
   "source": [
    "# Zet tokens om naar ID-nummers\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Token ‚Üí ID-nummer:\")\n",
    "print(\"-\" * 35)\n",
    "for token, id in zip(tokens, token_ids):\n",
    "    print(f\"  {token:15s} ‚Üí {id}\")\n",
    "\n",
    "print(f\"\\nBERT heeft een vocabulaire van {tokenizer.vocab_size:,} tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 3: Speciale tokens\n",
    "\n",
    "BERT voegt automatisch speciale tokens toe:\n",
    "- `[CLS]` ‚Äî markering van het begin van de tekst\n",
    "- `[SEP]` ‚Äî markering van het einde van de tekst\n",
    "- `[PAD]` ‚Äî opvulling om alle zinnen even lang te maken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volledige token-reeks (met padding tot lengte 20):\n",
      "--------------------------------------------------\n",
      "  [ 0] [CLS]            ID:   101  ‚úÖ\n",
      "  [ 1] p                ID:  1052  ‚úÖ\n",
      "  [ 2] ##yt             ID: 22123  ‚úÖ\n",
      "  [ 3] ##or             ID:  2953  ‚úÖ\n",
      "  [ 4] ##ch             ID:  2818  ‚úÖ\n",
      "  [ 5] and              ID:  1998  ‚úÖ\n",
      "  [ 6] hugging          ID: 17662  ‚úÖ\n",
      "  [ 7] face             ID:  2227  ‚úÖ\n",
      "  [ 8] make             ID:  2191  ‚úÖ\n",
      "  [ 9] deep             ID:  2784  ‚úÖ\n",
      "  [10] learning         ID:  4083  ‚úÖ\n",
      "  [11] simple           ID:  3722  ‚úÖ\n",
      "  [12] .                ID:  1012  ‚úÖ\n",
      "  [13] [SEP]            ID:   102  ‚úÖ\n",
      "  [14] [PAD]            ID:     0  ‚¨ú (padding)\n",
      "  [15] [PAD]            ID:     0  ‚¨ú (padding)\n",
      "  [16] [PAD]            ID:     0  ‚¨ú (padding)\n",
      "  [17] [PAD]            ID:     0  ‚¨ú (padding)\n",
      "  [18] [PAD]            ID:     0  ‚¨ú (padding)\n",
      "  [19] [PAD]            ID:     0  ‚¨ú (padding)\n",
      "\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "(1 = echte tekst, 0 = opvulling die het model negeert)\n"
     ]
    }
   ],
   "source": [
    "# Volledige encoding met speciale tokens en padding\n",
    "encoded = tokenizer(\n",
    "    tekst,\n",
    "    padding='max_length',  # Vul aan tot max_length\n",
    "    truncation=True,       # Knip af als het te lang is\n",
    "    max_length=20          # Maximale lengte\n",
    ")\n",
    "\n",
    "# Laat zien wat eruit komt\n",
    "alle_tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "\n",
    "print(\"Volledige token-reeks (met padding tot lengte 20):\")\n",
    "print(\"-\" * 50)\n",
    "for i, (token, id, mask) in enumerate(zip(\n",
    "    alle_tokens, encoded['input_ids'], encoded['attention_mask']\n",
    ")):\n",
    "    status = '‚úÖ' if mask == 1 else '‚¨ú (padding)'\n",
    "    print(f\"  [{i:2d}] {token:15s}  ID: {id:5d}  {status}\")\n",
    "\n",
    "print(f\"\\nAttention mask: {encoded['attention_mask']}\")\n",
    "print(\"(1 = echte tekst, 0 = opvulling die het model negeert)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 4: Terug van getallen naar tekst\n",
    "\n",
    "We kunnen de getallen ook weer terugvertalen naar tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origineel:  PyTorch and Hugging Face make deep learning simple.\n",
      "Gedecodeerd: pytorch and hugging face make deep learning simple.\n",
      "\n",
      "(Merk op: hoofdletters zijn verdwenen ‚Äî BERT werkt met 'uncased' tekst)\n"
     ]
    }
   ],
   "source": [
    "# Decodeer terug naar leesbare tekst\n",
    "decoded = tokenizer.decode(encoded['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Origineel:  {tekst}\")\n",
    "print(f\"Gedecodeerd: {decoded}\")\n",
    "print(f\"\\n(Merk op: hoofdletters zijn verdwenen ‚Äî BERT werkt met 'uncased' tekst)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Experimenteer zelf!\n",
    "\n",
    "Probeer verschillende zinnen te tokenizen. Wat gebeurt er met:\n",
    "- Nederlandse woorden?\n",
    "- Spelfouten?\n",
    "- Getallen?\n",
    "- Emoji's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekst:  Dit is een Nederlandse zin over kunstmatige intelligentie.\n",
      "Tokens: ['di', '##t', 'is', 'ee', '##n', 'ned', '##erland', '##se', 'z', '##in', 'over', 'kunst', '##mat', '##ige', 'intelligent', '##ie', '.']\n",
      "Aantal tokens: 17\n"
     ]
    }
   ],
   "source": [
    "# Probeer je eigen tekst!\n",
    "mijn_tekst = \"Dit is een Nederlandse zin over kunstmatige intelligentie.\"  # ‚Üê Pas dit aan\n",
    "\n",
    "mijn_tokens = tokenizer.tokenize(mijn_tekst)\n",
    "print(f\"Tekst:  {mijn_tekst}\")\n",
    "print(f\"Tokens: {mijn_tokens}\")\n",
    "print(f\"Aantal tokens: {len(mijn_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoe gaat de tokenizer om met spelfouten?\n",
      "--------------------------------------------------\n",
      "  intelligence         ‚Üí ['intelligence']\n",
      "  inteligence          ‚Üí ['intel', '##igen', '##ce']\n",
      "  inttellligennce      ‚Üí ['int', '##tell', '##li', '##gen', '##nce']\n",
      "  Amsterdam            ‚Üí ['amsterdam']\n",
      "  Amstrdm              ‚Üí ['am', '##st', '##rd', '##m']\n"
     ]
    }
   ],
   "source": [
    "# Vergelijk: correct gespeld vs. spelfout\n",
    "woorden = [\"intelligence\", \"inteligence\", \"inttellligennce\", \"Amsterdam\", \"Amstrdm\"]\n",
    "\n",
    "print(\"Hoe gaat de tokenizer om met spelfouten?\")\n",
    "print(\"-\" * 50)\n",
    "for woord in woorden:\n",
    "    toks = tokenizer.tokenize(woord)\n",
    "    print(f\"  {woord:20s} ‚Üí {toks}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
