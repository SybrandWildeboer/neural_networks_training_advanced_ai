{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìê Word Embeddings: Woorden als getallen\n",
    "\n",
    "Hoe begrijpt een AI-model de **betekenis** van woorden?\n",
    "\n",
    "Het antwoord: door elk woord om te zetten in een lijst van getallen ‚Äî een **vector**. Woorden met een vergelijkbare betekenis krijgen vergelijkbare vectoren.\n",
    "\n",
    "In dit notebook ontdek je:\n",
    "1. Hoe word embeddings werken\n",
    "2. Dat je er rekenkundig mee kunt werken (King - Man + Woman = ?)\n",
    "3. Hoe je vergelijkbare woorden kunt vinden\n",
    "\n",
    "---\n",
    "**Instructie:** Voer iedere cel uit met **Shift+Enter**\n",
    "\n",
    "‚ö†Ô∏è Het laden van het model (stap 1) duurt ~1-2 minuten bij de eerste keer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 1: Pre-trained model laden\n",
    "\n",
    "We laden een **GloVe model** dat is getraind op miljoenen teksten. Het model kent de vectorrepresentatie van 400.000 Engelse woorden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Laad een pre-trained model (400.000 woorden, 50-dimensionale vectoren)\n",
    "print(\"Model wordt geladen... (dit duurt ~1-2 minuten)\")\n",
    "model = api.load('glove-wiki-gigaword-50')\n",
    "print(f\"‚úÖ Model geladen! Vocabulaire: {len(model):,} woorden\")\n",
    "print(f\"   Elke vector heeft {model.vector_size} dimensies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 2: Hoe ziet een word embedding eruit?\n",
    "\n",
    "Elk woord wordt gerepresenteerd als een rij van 50 getallen. Elk getal staat voor een abstract concept dat het model heeft geleerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Bekijk de vector van een woord\n",
    "woord = \"king\"\n",
    "vector = model[woord]\n",
    "\n",
    "print(f\"Vector voor '{woord}':\")\n",
    "print(f\"  Lengte: {len(vector)} getallen\")\n",
    "print(f\"  Eerste 10 waarden: {np.round(vector[:10], 3)}\")\n",
    "print(f\"\\nDit is wat het neural network 'ziet' als het het woord '{woord}' leest.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 3: Vergelijkbare woorden vinden\n",
    "\n",
    "Omdat woorden met een vergelijkbare betekenis dicht bij elkaar liggen in de vectorruimte, kunnen we **vergelijkbare woorden** vinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoek woorden die vergelijkbaar zijn met een gegeven woord\n",
    "zoekwoord = \"happy\"  # ‚Üê Pas dit aan!\n",
    "\n",
    "vergelijkbaar = model.most_similar(zoekwoord, topn=8)\n",
    "\n",
    "print(f\"Woorden die lijken op '{zoekwoord}':\")\n",
    "print(\"-\" * 40)\n",
    "for woord, score in vergelijkbaar:\n",
    "    balk = '‚ñà' * int(score * 30)\n",
    "    print(f\"  {woord:15s} {score:.3f}  {balk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergelijk meerdere zoekwoorden\n",
    "zoekwoorden = [\"cat\", \"dog\", \"car\", \"computer\", \"king\"]\n",
    "\n",
    "for zoek in zoekwoorden:\n",
    "    top3 = model.most_similar(zoek, topn=3)\n",
    "    buren = ', '.join([w for w, _ in top3])\n",
    "    print(f\"  {zoek:12s} ‚Üí {buren}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 4: Rekenen met woorden üßÆ\n",
    "\n",
    "Dit is het krachtigste aspect van word embeddings: je kunt er **mee rekenen**!\n",
    "\n",
    "Het beroemdste voorbeeld:\n",
    "> **King - Man + Woman = ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# King - Man + Woman = ?\n",
    "resultaat = model.most_similar(\n",
    "    positive=['king', 'woman'], \n",
    "    negative=['man'], \n",
    "    topn=3\n",
    ")\n",
    "\n",
    "print(\"King - Man + Woman = ?\")\n",
    "print(\"-\" * 30)\n",
    "for woord, score in resultaat:\n",
    "    print(f\"  {woord:12s} ({score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meer voorbeelden van woordrekenkunde\n",
    "voorbeelden = [\n",
    "    (\"Paris\",   \"France\",  \"Germany\",  \"Parijs - Frankrijk + Duitsland = ?\"),\n",
    "    (\"walking\", \"walk\",    \"swim\",     \"Walking - Walk + Swim = ?\"),\n",
    "    (\"bigger\",  \"big\",     \"small\",    \"Bigger - Big + Small = ?\"),\n",
    "    (\"son\",     \"man\",     \"woman\",    \"Son - Man + Woman = ?\"),\n",
    "]\n",
    "\n",
    "for pos1, neg, pos2, beschrijving in voorbeelden:\n",
    "    result = model.most_similar(positive=[pos1, pos2], negative=[neg], topn=1)\n",
    "    antwoord, score = result[0]\n",
    "    print(f\"  {beschrijving:45s} ‚Üí {antwoord} ({score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 5: Visualisatie\n",
    "\n",
    "Laten we een groep woorden in 2D plotten om te zien hoe het model ze groepeert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Kies groepen woorden\n",
    "woord_groepen = {\n",
    "    'Royalty':  ['king', 'queen', 'prince', 'princess', 'throne'],\n",
    "    'Dieren':   ['cat', 'dog', 'horse', 'fish', 'bird'],\n",
    "    'Landen':   ['france', 'germany', 'italy', 'spain', 'japan'],\n",
    "    'Emoties':  ['happy', 'sad', 'angry', 'love', 'fear'],\n",
    "}\n",
    "\n",
    "alle_woorden = []\n",
    "kleuren = []\n",
    "kleur_map = {'Royalty': 'royalblue', 'Dieren': 'forestgreen', \n",
    "             'Landen': 'coral', 'Emoties': 'mediumpurple'}\n",
    "\n",
    "for groep, woorden in woord_groepen.items():\n",
    "    for w in woorden:\n",
    "        alle_woorden.append(w)\n",
    "        kleuren.append(kleur_map[groep])\n",
    "\n",
    "# Haal vectoren op en reduceer naar 2D met PCA\n",
    "vectoren = [model[w] for w in alle_woorden]\n",
    "pca = PCA(n_components=2)\n",
    "coords = pca.fit_transform(vectoren)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, woord in enumerate(alle_woorden):\n",
    "    plt.scatter(coords[i, 0], coords[i, 1], c=kleuren[i], s=100, zorder=2)\n",
    "    plt.annotate(woord, (coords[i, 0], coords[i, 1]), \n",
    "                fontsize=11, fontweight='bold',\n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "# Legenda\n",
    "for groep, kleur in kleur_map.items():\n",
    "    plt.scatter([], [], c=kleur, s=100, label=groep)\n",
    "plt.legend(fontsize=11, loc='best')\n",
    "\n",
    "plt.title('Word Embeddings in 2D ‚Äî Vergelijkbare woorden liggen bij elkaar', fontsize=13)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 6: Welk woord hoort er niet bij?\n",
    "\n",
    "Omdat het model de betekenis van woorden begrijpt, kan het ook bepalen welk woord **niet in een groep past**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welk woord hoort er niet bij?\n",
    "groepen = [\n",
    "    [\"cat\", \"dog\", \"horse\", \"computer\"],\n",
    "    [\"paris\", \"berlin\", \"london\", \"banana\"],\n",
    "    [\"happy\", \"sad\", \"angry\", \"table\"],\n",
    "    [\"doctor\", \"nurse\", \"surgeon\", \"bicycle\"],\n",
    "]\n",
    "\n",
    "print(\"Welk woord hoort er niet bij?\")\n",
    "print(\"-\" * 50)\n",
    "for groep in groepen:\n",
    "    vreemd = model.doesnt_match(groep)\n",
    "    print(f\"  {groep}  ‚Üí '{vreemd}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Experimenteer zelf!\n",
    "\n",
    "Probeer:\n",
    "- Andere woord-analogie√´n (bijv. `Tokyo - Japan + France = ?`)\n",
    "- Vergelijkbare woorden zoeken voor een woord naar keuze\n",
    "- Een eigen \"welk woord hoort er niet bij\" groep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probeer je eigen woordrekenkunde!\n",
    "resultaat = model.most_similar(\n",
    "    positive=['tokyo', 'france'],  # ‚Üê Pas dit aan\n",
    "    negative=['japan'],             # ‚Üê Pas dit aan\n",
    "    topn=3\n",
    ")\n",
    "\n",
    "print(\"Resultaat:\")\n",
    "for woord, score in resultaat:\n",
    "    print(f\"  {woord} ({score:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
