{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÇÔ∏è Tokenization: Hoe AI tekst leest\n",
    "\n",
    "Taalmodellen zoals ChatGPT, Claude en Gemini lezen geen woorden ‚Äî ze lezen **tokens**.\n",
    "\n",
    "Een token is een stukje tekst: soms een heel woord, soms een deel van een woord, soms een leesteken.\n",
    "\n",
    "In dit notebook ontdek je:\n",
    "1. Hoe tekst wordt opgesplitst in tokens\n",
    "2. Waarom sommige woorden in stukjes worden geknipt\n",
    "3. Hoe padding en truncation werken\n",
    "\n",
    "---\n",
    "**Instructie:** Voer iedere cel uit met **Shift+Enter**\n",
    "\n",
    "‚ö†Ô∏è De eerste cel kan even duren (~30 seconden) omdat er een tokenizer wordt gedownload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installeer de benodigde library (duurt even bij eerste keer)\n",
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 1: Tekst tokenizen\n",
    "\n",
    "We laden de **BERT tokenizer** ‚Äî dezelfde tokenizer die gebruikt wordt in veel AI-modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Laad de BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Probeer een zin\n",
    "tekst = \"PyTorch and Hugging Face make deep learning simple.\"\n",
    "\n",
    "# Tokenize!\n",
    "tokens = tokenizer.tokenize(tekst)\n",
    "\n",
    "print(f\"Originele tekst:  {tekst}\")\n",
    "print(f\"Aantal woorden:   {len(tekst.split())}\")\n",
    "print(f\"\\nTokens:           {tokens}\")\n",
    "print(f\"Aantal tokens:    {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wat valt je op?\n",
    "\n",
    "- `\"PyTorch\"` is opgesplitst in `['p', '##yt', '##or', '##ch']` ‚Äî het model kent het woord niet als geheel\n",
    "- `\"##\"` betekent: dit stukje hoort bij het vorige token\n",
    "- Bekende woorden zoals `\"and\"`, `\"make\"`, `\"deep\"` blijven heel\n",
    "- De punt `.` is een apart token\n",
    "\n",
    "## Stap 2: Van tokens naar getallen\n",
    "\n",
    "Het neural network werkt niet met tekst maar met **getallen**. Elk token heeft een uniek ID-nummer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zet tokens om naar ID-nummers\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Token ‚Üí ID-nummer:\")\n",
    "print(\"-\" * 35)\n",
    "for token, id in zip(tokens, token_ids):\n",
    "    print(f\"  {token:15s} ‚Üí {id}\")\n",
    "\n",
    "print(f\"\\nBERT heeft een vocabulaire van {tokenizer.vocab_size:,} tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 3: Speciale tokens\n",
    "\n",
    "BERT voegt automatisch speciale tokens toe:\n",
    "- `[CLS]` ‚Äî markering van het begin van de tekst\n",
    "- `[SEP]` ‚Äî markering van het einde van de tekst\n",
    "- `[PAD]` ‚Äî opvulling om alle zinnen even lang te maken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volledige encoding met speciale tokens en padding\n",
    "encoded = tokenizer(\n",
    "    tekst,\n",
    "    padding='max_length',  # Vul aan tot max_length\n",
    "    truncation=True,       # Knip af als het te lang is\n",
    "    max_length=20          # Maximale lengte\n",
    ")\n",
    "\n",
    "# Laat zien wat eruit komt\n",
    "alle_tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "\n",
    "print(\"Volledige token-reeks (met padding tot lengte 20):\")\n",
    "print(\"-\" * 50)\n",
    "for i, (token, id, mask) in enumerate(zip(\n",
    "    alle_tokens, encoded['input_ids'], encoded['attention_mask']\n",
    ")):\n",
    "    status = '‚úÖ' if mask == 1 else '‚¨ú (padding)'\n",
    "    print(f\"  [{i:2d}] {token:15s}  ID: {id:5d}  {status}\")\n",
    "\n",
    "print(f\"\\nAttention mask: {encoded['attention_mask']}\")\n",
    "print(\"(1 = echte tekst, 0 = opvulling die het model negeert)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 4: Terug van getallen naar tekst\n",
    "\n",
    "We kunnen de getallen ook weer terugvertalen naar tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodeer terug naar leesbare tekst\n",
    "decoded = tokenizer.decode(encoded['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Origineel:  {tekst}\")\n",
    "print(f\"Gedecodeerd: {decoded}\")\n",
    "print(f\"\\n(Merk op: hoofdletters zijn verdwenen ‚Äî BERT werkt met 'uncased' tekst)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Experimenteer zelf!\n",
    "\n",
    "Probeer verschillende zinnen te tokenizen. Wat gebeurt er met:\n",
    "- Nederlandse woorden?\n",
    "- Spelfouten?\n",
    "- Getallen?\n",
    "- Emoji's?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probeer je eigen tekst!\n",
    "mijn_tekst = \"Dit is een Nederlandse zin over kunstmatige intelligentie.\"  # ‚Üê Pas dit aan\n",
    "\n",
    "mijn_tokens = tokenizer.tokenize(mijn_tekst)\n",
    "print(f\"Tekst:  {mijn_tekst}\")\n",
    "print(f\"Tokens: {mijn_tokens}\")\n",
    "print(f\"Aantal tokens: {len(mijn_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergelijk: correct gespeld vs. spelfout\n",
    "woorden = [\"intelligence\", \"inteligence\", \"inttellligennce\", \"Amsterdam\", \"Amstrdm\"]\n",
    "\n",
    "print(\"Hoe gaat de tokenizer om met spelfouten?\")\n",
    "print(\"-\" * 50)\n",
    "for woord in woorden:\n",
    "    toks = tokenizer.tokenize(woord)\n",
    "    print(f\"  {woord:20s} ‚Üí {toks}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
